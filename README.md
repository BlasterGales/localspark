# Ollama Spark - Local AI Development Assistant

Una interfaz local elegante para interactuar con modelos de Ollama, dise√±ada espec√≠ficamente para desarrolladores.

## üöÄ Caracter√≠sticas

- **Interfaz moderna**: Dise√±o limpio inspirado en GitHub Spark
- **Soporte completo de Ollama**: Conexi√≥n directa con tu instalaci√≥n local
- **Contexto de proyecto**: Sube archivos para proporcionar contexto al AI
- **Streaming en tiempo real**: Respuestas fluidas mientras el AI piensa
- **Gesti√≥n robusta de errores**: Manejo gracioso de desconexiones y errores
- **Persistencia de datos**: Conversaciones y archivos se guardan autom√°ticamente

## üìã Prerrequisitos

### 1. Node.js y npm
```bash
# Verificar instalaci√≥n
node --version  # Debe ser v18 o superior
npm --version   # Debe ser v8 o superior
```

Si no tienes Node.js instalado:
- **Windows/Mac**: Descarga desde [nodejs.org](https://nodejs.org/)
- **Linux (Ubuntu/Debian)**:
  ```bash
  curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
  sudo apt-get install -y nodejs
  ```

### 2. Ollama
Ollama debe estar instalado y ejecut√°ndose en tu sistema.

#### Instalaci√≥n de Ollama:

**macOS:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Descarga el instalador desde [ollama.ai](https://ollama.ai/download)

#### Verificar instalaci√≥n de Ollama:
```bash
ollama --version
```

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### 1. Clonar el repositorio
```bash
git clone <URL_DEL_REPOSITORIO>
cd ollama-spark
```

### 2. Instalar dependencias
```bash
npm install
```

### 3. Verificar que Ollama est√© ejecut√°ndose
```bash
# Iniciar Ollama (si no est√° ejecut√°ndose)
ollama serve

# En otra terminal, verificar la conexi√≥n
curl http://localhost:11434/api/tags
```

Si ves una respuesta JSON, Ollama est√° funcionando correctamente.

### 4. Descargar modelos de AI

**Modelos recomendados para desarrollo:**

```bash
# Modelo peque√±o y r√°pido (ideal para desarrollo)
ollama pull llama2:7b

# Modelo m√°s capaz (requiere m√°s recursos)
ollama pull codellama:13b

# Modelo especializado en c√≥digo
ollama pull codellama:python

# Modelo muy ligero para pruebas
ollama pull tinyllama
```

**Para verificar modelos instalados:**
```bash
ollama list
```

## üèÉ‚Äç‚ôÇÔ∏è Ejecutar la Aplicaci√≥n

### Modo Desarrollo
```bash
npm run dev
```

La aplicaci√≥n estar√° disponible en: `http://localhost:5173`

### Modo Producci√≥n
```bash
# Construir la aplicaci√≥n
npm run build

# Servir la aplicaci√≥n construida
npm run preview
```

## üîß Resoluci√≥n de Problemas

### Error: "Cannot connect to Ollama"

**1. Verificar que Ollama est√© ejecut√°ndose:**
```bash
# Comprobar proceso
ps aux | grep ollama

# Si no est√° ejecut√°ndose, iniciarlo
ollama serve
```

**2. Verificar puerto:**
```bash
# Ollama debe ejecutarse en puerto 11434
netstat -tlnp | grep 11434
```

**3. Verificar conexi√≥n:**
```bash
curl http://localhost:11434/api/tags
```

### Error: "No models available"

**Descargar al menos un modelo:**
```bash
ollama pull llama2:7b
```

**Verificar modelos descargados:**
```bash
ollama list
```

### Puerto ocupado (Error EADDRINUSE)

**Cambiar puerto de desarrollo:**
```bash
# Editar package.json o usar variable de entorno
PORT=3001 npm run dev
```

### Problemas de rendimiento

**Para modelos grandes:**
- Aseg√∫rate de tener suficiente RAM (8GB+ recomendado)
- Cierra otras aplicaciones pesadas
- Considera usar modelos m√°s peque√±os (`tinyllama`, `llama2:7b`)

**Optimizar Ollama:**
```bash
# Configurar l√≠mite de memoria (ejemplo: 4GB)
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_MAX_QUEUE=1
```

## üéØ Uso de la Aplicaci√≥n

### 1. Primera vez
1. Abre `http://localhost:5173`
2. Espera a que se conecte con Ollama
3. Selecciona un modelo del dropdown
4. ¬°Comienza a chatear!

### 2. Subir archivos de proyecto
1. Arrastra archivos a la barra lateral izquierda
2. O usa el bot√≥n "Choose Files"
3. Los archivos proporcionan contexto al AI

### 3. Tipos de archivos soportados
- JavaScript/TypeScript (`.js`, `.jsx`, `.ts`, `.tsx`)
- Estilos (`.css`, `.scss`, `.less`)
- Markup (`.html`, `.xml`)
- Datos (`.json`, `.yaml`, `.yml`)
- Documentaci√≥n (`.md`, `.txt`)
- C√≥digo (`.py`, `.java`, `.cpp`, `.c`, `.h`)

### 4. Comandos √∫tiles
- **Enter**: Enviar mensaje
- **Shift + Enter**: Nueva l√≠nea
- **Ctrl/Cmd + R**: Refrescar conexi√≥n

## üîí Seguridad y Privacidad

- **100% Local**: Toda la comunicaci√≥n es local, nada se env√≠a a internet
- **Sin telemetr√≠a**: No se recopilan datos de uso
- **Datos persistentes**: Conversaciones se guardan localmente en tu navegador
- **Archivos seguros**: Los archivos subidos solo existen en tu sesi√≥n local

## üõ†Ô∏è Desarrollo y Personalizaci√≥n

### Estructura del proyecto
```
src/
‚îú‚îÄ‚îÄ components/          # Componentes React
‚îú‚îÄ‚îÄ lib/                # Utilidades y API
‚îú‚îÄ‚îÄ assets/             # Recursos est√°ticos
‚îî‚îÄ‚îÄ styles/             # Estilos globales
```

### Scripts disponibles
```bash
npm run dev             # Servidor de desarrollo
npm run build           # Construir para producci√≥n
npm run preview         # Vista previa de build
npm run lint            # Linter de c√≥digo
npm run type-check      # Verificaci√≥n de tipos
```

### Personalizar tema
Edita `src/index.css` para cambiar colores y estilos.

## üìö Comandos de Ollama √ötiles

```bash
# Gesti√≥n de modelos
ollama list                          # Listar modelos instalados
ollama pull <modelo>                 # Descargar modelo
ollama rm <modelo>                   # Eliminar modelo
ollama show <modelo>                 # Informaci√≥n del modelo

# Administraci√≥n
ollama serve                         # Iniciar servidor
ollama ps                           # Procesos activos
ollama stop <modelo>                # Detener modelo espec√≠fico

# Ejemplos de uso directo
ollama run llama2:7b "Hello world"  # Ejecutar prompt directo
```

## ü§ù Soporte

Si encuentras problemas:

1. **Verifica prerrequisitos**: Node.js, Ollama instalados y funcionando
2. **Consulta logs**: Abre DevTools del navegador (F12) y revisa la consola
3. **Reinicia servicios**: Det√©n y reinicia tanto Ollama como la aplicaci√≥n
4. **Verifica recursos**: Aseg√∫rate de tener suficiente RAM disponible

## üìù Notas Adicionales

- La primera ejecuci√≥n puede ser lenta mientras los modelos se cargan
- Los modelos m√°s grandes ofrecen mejores respuestas pero requieren m√°s recursos
- Guarda tus conversaciones importantes, ya que se almacenan localmente
- Puedes ejecutar m√∫ltiples instancias de Ollama con diferentes configuraciones

---

**¬°Disfruta desarrollando con tu asistente AI local!** üöÄ